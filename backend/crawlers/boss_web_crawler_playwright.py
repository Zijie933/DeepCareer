"""
BOSSç›´è˜ç½‘é¡µçˆ¬è™« - åŸºäºPlaywrightï¼ˆæ— éœ€Cookieï¼Œå¤šUAè½®æ¢ï¼‰
"""
import asyncio
import random
import time
from typing import List, Dict, Optional
from playwright.async_api import async_playwright, Page, Browser
from backend.utils.logger import logger


class UserAgentPool:
    """User-Agentæ± ï¼ˆåŒ…å«ä¸»æµæµè§ˆå™¨çš„çœŸå®UAï¼‰"""
    
    USER_AGENTS = [
        # Chrome (Windows)
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
        
        # Chrome (macOS)
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
        
        # Firefox (Windows)
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
        
        # Firefox (macOS)
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0",
        
        # Safari (macOS)
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
        
        # Edge (Windows)
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
    ]
    
    @classmethod
    def get_random(cls) -> str:
        """è·å–éšæœºUA"""
        return random.choice(cls.USER_AGENTS)


class RateLimiter:
    """è¯·æ±‚é™æµå™¨"""
    
    def __init__(self, min_interval: float = 3.0, max_interval: float = 8.0):
        self.min_interval = min_interval
        self.max_interval = max_interval
        self.last_request_time = 0
        self._lock = asyncio.Lock()
    
    async def wait(self):
        """ç­‰å¾…åˆ°å¯ä»¥å‘é€ä¸‹ä¸€ä¸ªè¯·æ±‚"""
        async with self._lock:
            now = time.time()
            elapsed = now - self.last_request_time
            
            required_wait = random.uniform(self.min_interval, self.max_interval)
            wait_time = max(0, required_wait - elapsed)
            
            if wait_time > 0:
                logger.debug(f"â±ï¸  é™æµç­‰å¾… {wait_time:.2f} ç§’...")
                await asyncio.sleep(wait_time)
            
            self.last_request_time = time.time()


class BossWebCrawlerPlaywright:
    """BOSSç›´è˜ç½‘é¡µçˆ¬è™«ï¼ˆPlaywrightç‰ˆï¼Œæ— éœ€Cookieï¼Œå¤šUAè½®æ¢ï¼‰"""
    
    BASE_URL = "https://www.zhipin.com"
    SEARCH_URL = "https://www.zhipin.com/web/geek/jobs"
    
    CITY_CODES = {
        # å…¨å›½
        "å…¨å›½": "100010000",
        
        # ä¸€çº¿åŸå¸‚
        "åŒ—äº¬": "101010100",
        "ä¸Šæµ·": "101020100",
        "å¹¿å·": "101280100",
        "æ·±åœ³": "101280600",
        
        # æ–°ä¸€çº¿åŸå¸‚
        "æ­å·": "101210100",
        "æˆéƒ½": "101270100",
        "é‡åº†": "101040100",
        "æ­¦æ±‰": "101200100",
        "è¥¿å®‰": "101110100",
        "è‹å·": "101190400",
        "å—äº¬": "101190100",
        "å¤©æ´¥": "101030100",
        "éƒ‘å·": "101180100",
        "é•¿æ²™": "101250100",
        "ä¸œè": "101281600",
        "ä½›å±±": "101280800",
        "å®æ³¢": "101210400",
        "é’å²›": "101120200",
        "æ²ˆé˜³": "101070100",
        
        # äºŒçº¿åŸå¸‚
        "åˆè‚¥": "101220100",
        "å¦é—¨": "101230200",
        "æ— é”¡": "101190200",
        "æ˜†æ˜": "101290100",
        "å¤§è¿": "101070200",
        "ç¦å·": "101230100",
        "å“ˆå°”æ»¨": "101050100",
        "æµå—": "101120100",
        "æ¸©å·": "101210700",
        "çŸ³å®¶åº„": "101090100",
        "å—å®": "101300100",
        "é•¿æ˜¥": "101060100",
        "æ³‰å·": "101230500",
        "è´µé˜³": "101260100",
        "å—æ˜Œ": "101240100",
        "é‡‘å": "101210900",
        "å¸¸å·": "101191100",
        "ç æµ·": "101280700",
        "æƒ å·": "101280300",
        "å˜‰å…´": "101210300",
        "å—é€š": "101190500",
        "ä¸­å±±": "101281700",
        "å¤ªåŸ": "101100100",
        "å…°å·": "101160100",
        "å¾å·": "101190800",
        "å°å·": "101210600",
        "ç»å…´": "101210500",
        "çƒŸå°": "101120500",
        "æµ·å£": "101310100",
        
        # å…¶ä»–åŸå¸‚
        "ä¹Œé²æœ¨é½": "101130100",
        "å‘¼å’Œæµ©ç‰¹": "101080100",
        "é“¶å·": "101170100",
        "è¥¿å®": "101150100",
        "æ‹‰è¨": "101140100",
        "ä¸‰äºš": "101310200",
    }
    
    def __init__(
        self,
        min_delay: float = 3.0,
        max_delay: float = 8.0,
        max_retries: int = 3,
        retry_backoff: float = 15.0,
        timeout: float = 30000,  # Playwrightä½¿ç”¨æ¯«ç§’
        headless: bool = True,
        cookie_string: Optional[str] = None,  # Cookieå­—ç¬¦ä¸²
        target_city: Optional[str] = None  # æ–°å¢ï¼šç›®æ ‡åŸå¸‚ï¼ˆç”¨äºæ›¿æ¢Cookieä¸­çš„lastCityï¼‰
    ):
        self.min_delay = min_delay
        self.max_delay = max_delay
        self.max_retries = max_retries
        self.retry_backoff = retry_backoff
        self.timeout = timeout
        self.headless = headless
        self.cookie_string = cookie_string
        self.target_city = target_city
        
        self.rate_limiter = RateLimiter(min_delay, max_delay)
        
        self.playwright = None
        self.browser = None
        self.context = None
        
        self.stats = {
            "total_requests": 0,
            "success_requests": 0,
            "failed_requests": 0,
            "retried_requests": 0,
            "user_agents_used": set(),
        }
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=self.headless,
            args=['--disable-blink-features=AutomationControlled']
        )
        
        # åˆ›å»ºä¸Šä¸‹æ–‡ï¼ˆéšæœºUAï¼‰
        user_agent = UserAgentPool.get_random()
        self.stats["user_agents_used"].add(user_agent)
        
        self.context = await self.browser.new_context(
            user_agent=user_agent,
            viewport={'width': 1920, 'height': 1080},
            locale='zh-CN'
        )
        
        # å¦‚æœæä¾›äº†Cookieï¼Œåˆ™è®¾ç½®Cookie
        if self.cookie_string:
            # å¦‚æœæŒ‡å®šäº†ç›®æ ‡åŸå¸‚ï¼Œæ›¿æ¢lastCity
            target_city_code = None
            if self.target_city and self.target_city in self.CITY_CODES:
                target_city_code = self.CITY_CODES[self.target_city]
                logger.info(f"ğŸ”„ å°†Cookieä¸­çš„lastCityæ›¿æ¢ä¸º: {self.target_city} ({target_city_code})")
            
            cookies = self._parse_cookie_string(self.cookie_string, target_city_code)
            await self.context.add_cookies(cookies)
            logger.info(f"ğŸª å·²è®¾ç½® {len(cookies)} ä¸ªCookie")
        
        # æ³¨å…¥åæ£€æµ‹è„šæœ¬
        await self.context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)
        
        logger.info(f"ğŸŒ æµè§ˆå™¨å·²å¯åŠ¨ (UA: {user_agent[:50]}...)")
        return self
    
    def _parse_cookie_string(self, cookie_string: str, target_city_code: Optional[str] = None) -> List[Dict]:
        """
        è§£æCookieå­—ç¬¦ä¸²ä¸ºPlaywrightæ ¼å¼
        
        Args:
            cookie_string: Cookieå­—ç¬¦ä¸²
            target_city_code: ç›®æ ‡åŸå¸‚ä»£ç ï¼Œå¦‚æœæä¾›åˆ™æ›¿æ¢lastCity
        
        Returns:
            Cookieå­—å…¸åˆ—è¡¨
        """
        cookies = []
        for item in cookie_string.split('; '):
            if '=' in item:
                name, value = item.split('=', 1)
                
                # å¦‚æœæŒ‡å®šäº†ç›®æ ‡åŸå¸‚ä»£ç ï¼Œæ›¿æ¢lastCity
                if name == 'lastCity' and target_city_code:
                    value = target_city_code
                
                cookies.append({
                    'name': name,
                    'value': value,
                    'domain': '.zhipin.com',
                    'path': '/'
                })
        
        return cookies
    
    def update_cookie_city(self, city: str):
        """
        æ›´æ–°Cookieä¸­çš„lastCityä¸ºç›®æ ‡åŸå¸‚
        
        Args:
            city: åŸå¸‚åç§°
        """
        if not self.cookie_string:
            return
        
        city_code = self.CITY_CODES.get(city)
        if not city_code:
            logger.warning(f"æœªæ‰¾åˆ°åŸå¸‚ {city} çš„ä»£ç ")
            return
        
        # æ›¿æ¢Cookieä¸­çš„lastCity
        import re
        self.cookie_string = re.sub(
            r'lastCity=\d+',
            f'lastCity={city_code}',
            self.cookie_string
        )
        logger.info(f"ğŸ”„ å·²æ›´æ–°Cookieä¸­çš„lastCityä¸º: {city_code}")
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
        
        logger.info(f"ğŸ”’ æµè§ˆå™¨å·²å…³é—­ - ç»Ÿè®¡: {self.get_stats()}")
    
    async def _parse_job_cards(self, page: Page) -> List[Dict]:
        """
        è§£æèŒä½å¡ç‰‡ï¼ˆåˆ—è¡¨é¡µï¼‰
        
        æ ¹æ®å®é™…HTMLç»“æ„ï¼š
        - ul.rec-job-list: èŒä½åˆ—è¡¨å®¹å™¨
        - li.job-card-box: æ¯ä¸ªèŒä½å¡ç‰‡
        - .job-info > .job-title > a.job-name: èŒä½åç§°å’Œé“¾æ¥
        - .job-salary: è–ªèµ„
        - .tag-list > li: ç»éªŒã€å­¦å†ç­‰æ ‡ç­¾
        - .job-card-footer: å…¬å¸ä¿¡æ¯
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
        
        Returns:
            èŒä½åˆ—è¡¨ï¼ˆåŸºç¡€ä¿¡æ¯+è¯¦æƒ…é¡µé“¾æ¥ï¼‰
        """
        jobs = []
        
        try:
            # ç­‰å¾…èŒä½åˆ—è¡¨åŠ è½½
            await page.wait_for_selector('ul.rec-job-list', timeout=15000)
            logger.debug("âœ… èŒä½åˆ—è¡¨å·²åŠ è½½")
            
            # è·å–æ‰€æœ‰èŒä½å¡ç‰‡
            job_cards = await page.query_selector_all('li.job-card-box')
            
            logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(job_cards)} ä¸ªèŒä½å¡ç‰‡")
            
            for idx, card in enumerate(job_cards, 1):
                try:
                    # 1. èŒä½åç§°å’Œé“¾æ¥ï¼ˆåœ¨ .job-info > .job-title > a.job-nameï¼‰
                    job_name_link = await card.query_selector('a.job-name')
                    if not job_name_link:
                        logger.debug(f"âš ï¸  å¡ç‰‡ {idx} æ—  job-name é“¾æ¥ï¼Œè·³è¿‡")
                        continue
                    
                    title = await job_name_link.text_content()
                    href = await job_name_link.get_attribute('href')
                    
                    if not href:
                        logger.debug(f"âš ï¸  å¡ç‰‡ {idx} hrefä¸ºç©ºï¼Œè·³è¿‡")
                        continue
                    
                    # æ„å»ºå®Œæ•´URL
                    job_url = self.BASE_URL + href if not href.startswith('http') else href
                    
                    # æå–job_id
                    job_id = ""
                    parts = href.split('/')
                    if len(parts) > 0:
                        job_id = parts[-1].replace('.html', '').split('?')[0]
                    
                    # 2. è–ªèµ„
                    salary_elem = await card.query_selector('.job-salary')
                    salary = await salary_elem.text_content() if salary_elem else ""
                    
                    # 3. æ ‡ç­¾ï¼ˆç»éªŒã€å­¦å†ç­‰ï¼‰
                    tag_list = await card.query_selector_all('ul.tag-list > li')
                    tags = []
                    for tag_elem in tag_list:
                        tag_text = await tag_elem.text_content()
                        if tag_text:
                            tags.append(tag_text.strip())
                    
                    # è§£ææ ‡ç­¾ï¼ˆé€šå¸¸ç¬¬ä¸€ä¸ªæ˜¯ç»éªŒï¼Œç¬¬äºŒä¸ªæ˜¯å­¦å†ï¼‰
                    experience = tags[0] if len(tags) > 0 else "ç»éªŒä¸é™"
                    education = tags[1] if len(tags) > 1 else "å­¦å†ä¸é™"
                    
                    # 4. å…¬å¸ä¿¡æ¯ï¼ˆåœ¨ .job-card-footerï¼‰
                    footer = await card.query_selector('.job-card-footer')
                    company = ""
                    location = ""
                    
                    if footer:
                        # å…¬å¸ç®€ç§°ï¼ˆä» .boss-name æå–ï¼Œè¯¦æƒ…é¡µä¼šç”¨æ›´å‡†ç¡®çš„å€¼è¦†ç›–ï¼‰
                        company_elem = await footer.query_selector('.boss-name')
                        if company_elem:
                            company = await company_elem.text_content()
                        
                        # å·¥ä½œåœ°ç‚¹
                        location_elem = await footer.query_selector('.company-location')
                        if location_elem:
                            location = await location_elem.text_content()
                    
                    job_data = {
                        "job_id": job_id,
                        "title": title.strip() if title else "",
                        "company": company.strip() if company else "",
                        "salary": salary.strip() if salary else "",
                        "location": location.strip() if location else "",
                        "experience": experience,
                        "education": education,
                        "tags": tags,
                        "job_url": job_url,
                        "platform": "boss",
                    }
                    
                    jobs.append(job_data)
                    logger.debug(f"âœ… è§£æèŒä½ {idx}: {job_data['title']} @ {job_data['company']}")
                
                except Exception as e:
                    logger.error(f"âŒ è§£æèŒä½å¡ç‰‡ {idx} å¤±è´¥: {e}")
                    continue
        
        except Exception as e:
            logger.error(f"âŒ ç­‰å¾…èŒä½åˆ—è¡¨è¶…æ—¶æˆ–è§£æå¤±è´¥: {e}")
            # ä¿å­˜æˆªå›¾å’ŒHTMLç”¨äºè°ƒè¯•
            try:
                await page.screenshot(path="debug_list_page.png")
                html_content = await page.content()
                with open("debug_list_page.html", "w", encoding="utf-8") as f:
                    f.write(html_content)
                logger.info("ğŸ’¾ è°ƒè¯•æ–‡ä»¶å·²ä¿å­˜: debug_list_page.png å’Œ debug_list_page.html")
            except:
                pass
        
        return jobs
    
    async def search_jobs(
        self,
        keyword: str,
        city: str = "æ·±åœ³",
        page: int = 1,
        auto_scroll: bool = True,
        max_scroll: int = 5
    ) -> List[Dict]:
        """
        æœç´¢èŒä½ï¼ˆä½¿ç”¨Playwrightæ¸²æŸ“é¡µé¢ï¼‰
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            city: åŸå¸‚åç§°
            page: é¡µç 
            auto_scroll: æ˜¯å¦è‡ªåŠ¨æ»šåŠ¨åŠ è½½æ›´å¤šï¼ˆéœ€è¦Cookieæ”¯æŒï¼‰
            max_scroll: æœ€å¤§æ»šåŠ¨æ¬¡æ•°
        
        Returns:
            èŒä½åˆ—è¡¨
        """
        city_code = self.CITY_CODES.get(city, self.CITY_CODES["æ·±åœ³"])
        
        url = f"{self.SEARCH_URL}?query={keyword}&city={city_code}&page={page}"
        
        logger.info(f"ğŸ” æœç´¢èŒä½: keyword={keyword}, city={city}, page={page}")
        
        try:
            # é™æµç­‰å¾…
            await self.rate_limiter.wait()
            
            self.stats["total_requests"] += 1
            
            # åˆ›å»ºæ–°é¡µé¢
            page_obj = await self.context.new_page()
            
            try:
                # è®¿é—®é¡µé¢
                logger.info(f"ğŸŒ è®¿é—®: {url}")
                await page_obj.goto(url, wait_until='domcontentloaded', timeout=self.timeout)
                
                # ç­‰å¾…JavaScriptæ¸²æŸ“
                logger.debug("â±ï¸  ç­‰å¾…é¡µé¢æ¸²æŸ“...")
                await asyncio.sleep(random.uniform(3, 5))
                
                # å¦‚æœå¯ç”¨è‡ªåŠ¨æ»šåŠ¨ä¸”æœ‰Cookie
                if auto_scroll and self.cookie_string:
                    try:
                        initial_count = len(await page_obj.query_selector_all('li.job-card-box'))
                        logger.info(f"ğŸ“œ å¼€å§‹æ»šåŠ¨åŠ è½½æ›´å¤šèŒä½ï¼ˆåˆå§‹: {initial_count}ï¼‰...")
                        
                        for i in range(max_scroll):
                            # æ£€æŸ¥é¡µé¢æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
                            if page_obj.is_closed():
                                logger.warning("âš ï¸ é¡µé¢å·²å…³é—­ï¼Œåœæ­¢æ»šåŠ¨")
                                break
                            
                            await page_obj.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                            await asyncio.sleep(random.uniform(1.5, 2.5))
                            
                            # æ£€æŸ¥æ˜¯å¦å‘ç”Ÿå¯¼èˆª
                            if page_obj.is_closed():
                                logger.warning("âš ï¸ é¡µé¢åœ¨æ»šåŠ¨åå…³é—­ï¼Œåœæ­¢æ»šåŠ¨")
                                break
                            
                            try:
                                current_count = len(await page_obj.query_selector_all('li.job-card-box'))
                                if current_count > initial_count:
                                    logger.debug(f"  æ»šåŠ¨ {i+1}/{max_scroll}: æ–°å¢ {current_count - initial_count} ä¸ªèŒä½")
                                    initial_count = current_count
                                else:
                                    logger.debug(f"  æ»šåŠ¨ {i+1}/{max_scroll}: æ²¡æœ‰æ–°å¢èŒä½")
                            except Exception as scroll_err:
                                logger.warning(f"âš ï¸ æ»šåŠ¨æ—¶å‡ºé”™: {scroll_err}ï¼Œåœæ­¢æ»šåŠ¨")
                                break
                    except Exception as scroll_init_err:
                        logger.warning(f"âš ï¸ åˆå§‹åŒ–æ»šåŠ¨å¤±è´¥: {scroll_init_err}ï¼Œè·³è¿‡æ»šåŠ¨")
                
                # è§£æèŒä½
                jobs = await self._parse_job_cards(page_obj)
                
                self.stats["success_requests"] += 1
                logger.info(f"âœ… æˆåŠŸè§£æ {len(jobs)} ä¸ªèŒä½")
                
                return jobs
            
            finally:
                await page_obj.close()
        
        except Exception as e:
            logger.error(f"âŒ æœç´¢èŒä½å¤±è´¥: {e}")
            self.stats["failed_requests"] += 1
            return []
    
    async def get_job_detail(self, job_url: str, use_random_ua: bool = False) -> Optional[Dict]:
        """
        è·å–èŒä½è¯¦æƒ…é¡µçš„å®Œæ•´ä¿¡æ¯
        
        æ ¹æ®HTMLç»“æ„ï¼š
        - job-primary detail-box: æ‹›è˜å²—ä½åŸºæœ¬ä¿¡æ¯ï¼ˆå²—ä½åã€å·¥èµ„ã€å­¦å†ç­‰ï¼‰
        - job-detail-section: èŒä½æè¿°åŒºåŸŸ
          - job-sec-text: èŒä½æè¿°æ–‡æœ¬
          - job-keyword-list: èŒä½æ ‡ç­¾
        - job-detail-section job-detail-company: å…¬å¸ä¿¡æ¯
          - detail-section-item company-info-box: å…¬å¸ä»‹ç»
          - detail-section-item company-address: å·¥ä½œåœ°å€
        
        Args:
            job_url: èŒä½è¯¦æƒ…URL
            use_random_ua: æ˜¯å¦ä¸ºæ­¤è¯·æ±‚ä½¿ç”¨éšæœºUser-Agentï¼ˆç”¨äºå¹¶å‘æ—¶å‡å°‘ç‰¹å¾ï¼‰
        
        Returns:
            èŒä½è¯¦æƒ…å­—å…¸
        """
        # logger.debug(f"ğŸ“„ è·å–èŒä½è¯¦æƒ…: {job_url}")
        
        try:
            # é™æµç­‰å¾…
            await self.rate_limiter.wait()
            
            self.stats["total_requests"] += 1
            
            # å¦‚æœå¯ç”¨éšæœºUAï¼Œåˆ›å»ºæ–°çš„context
            if use_random_ua:
                user_agent = UserAgentPool.get_random()
                self.stats["user_agents_used"].add(user_agent)
                
                # åˆ›å»ºä¸´æ—¶context
                temp_context = await self.browser.new_context(
                    user_agent=user_agent,
                    viewport={'width': 1920, 'height': 1080},
                    locale='zh-CN'
                )
                
                # å¦‚æœæœ‰Cookieï¼Œä¹Ÿè®¾ç½®åˆ°ä¸´æ—¶context
                if self.cookie_string:
                    target_city_code = None
                    if self.target_city and self.target_city in self.CITY_CODES:
                        target_city_code = self.CITY_CODES[self.target_city]
                    cookies = self._parse_cookie_string(self.cookie_string, target_city_code)
                    await temp_context.add_cookies(cookies)
                
                # æ³¨å…¥åæ£€æµ‹è„šæœ¬
                await temp_context.add_init_script("""
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    });
                """)
                
                page = await temp_context.new_page()
                should_close_context = True
            else:
                page = await self.context.new_page()
                temp_context = None
                should_close_context = False
            
            try:
                await page.goto(job_url, wait_until='domcontentloaded', timeout=self.timeout)
                
                # ç­‰å¾…è¯¦æƒ…é¡µåŠ è½½
                await page.wait_for_selector('.job-primary', timeout=15000)
                logger.debug("âœ… èŒä½è¯¦æƒ…é¡µå·²åŠ è½½")
                
                # é¢å¤–ç­‰å¾…JSæ¸²æŸ“
                await asyncio.sleep(random.uniform(2, 4))
                
                detail = {}
                
                # ========== 1. èŒä½åŸºæœ¬ä¿¡æ¯ï¼ˆjob-primaryï¼‰==========
                primary_section = await page.query_selector('.job-primary')
                if primary_section:
                    # èŒä½åç§°ï¼ˆh1æ ‡ç­¾ï¼‰
                    title_h1 = await primary_section.query_selector('.name h1')
                    if title_h1:
                        detail['job_title'] = (await title_h1.text_content()).strip()
                    
                    # è–ªèµ„ï¼ˆç‹¬ç«‹çš„.salaryå…ƒç´ ï¼‰
                    salary_elem = await primary_section.query_selector('.salary')
                    if salary_elem:
                        detail['salary_detail'] = (await salary_elem.text_content()).strip()
                    
                    # æ ‡ç­¾ä¿¡æ¯ï¼ˆåˆ†åˆ«æå–åœ°åŒºã€ç»éªŒã€å­¦å†ï¼‰
                    # åœ°åŒº
                    city_elem = await primary_section.query_selector('.text-desc.text-city')
                    if city_elem:
                        detail['work_city'] = (await city_elem.text_content()).strip()
                    
                    # ç»éªŒï¼ˆæ³¨æ„æ‹¼å†™æ˜¯experieceä¸æ˜¯experienceï¼‰
                    exp_elem = await primary_section.query_selector('.text-desc.text-experiece')
                    if exp_elem:
                        detail['experience_requirement'] = (await exp_elem.text_content()).strip()
                    
                    # å­¦å†
                    degree_elem = await primary_section.query_selector('.text-desc.text-degree')
                    if degree_elem:
                        detail['education_requirement'] = (await degree_elem.text_content()).strip()
                
                # ========== 2. èŒä½æè¿°ï¼ˆjob-detail-sectionï¼‰==========
                # èŒä½æè¿°æ–‡æœ¬
                job_desc_elem = await page.query_selector('.job-sec-text')
                if job_desc_elem:
                    desc_text = await job_desc_elem.text_content()
                    detail['job_description'] = desc_text.strip() if desc_text else ""
                else:
                    detail['job_description'] = ""
                
                # èŒä½æ ‡ç­¾/å…³é”®è¯
                keyword_list = await page.query_selector_all('.job-keyword-list > li')
                keywords = []
                for kw in keyword_list:
                    kw_text = await kw.text_content()
                    if kw_text:
                        keywords.append(kw_text.strip())
                detail['job_keywords'] = keywords
                
                # ========== 3. å…¬å¸ä¿¡æ¯ï¼ˆjob-detail-companyï¼‰==========
                company_section = await page.query_selector('.job-detail-company')
                if company_section:
                    # å…¬å¸ç®€ç§°ï¼ˆä» ka="job-detail-company_custompage" æå–ï¼Œè¦†ç›–åˆ—è¡¨é¡µçš„å€¼ï¼‰
                    company_short_elem = await company_section.query_selector('[ka="job-detail-company_custompage"]')
                    if company_short_elem:
                        detail['company'] = (await company_short_elem.text_content()).strip()
                    
                    # å…¬å¸å…¨ç§°ï¼ˆä»å·¥å•†ä¿¡æ¯çš„ li.company-name æå–ï¼‰
                    business_info_box = await company_section.query_selector('.business-info-box')
                    if business_info_box:
                        company_name_li = await business_info_box.query_selector('li.company-name')
                        if company_name_li:
                            # è·å–å®Œæ•´æ–‡æœ¬ï¼Œç„¶åç§»é™¤"å…¬å¸åç§°"æ ‡ç­¾
                            full_text = (await company_name_li.text_content()).strip()
                            # ç§»é™¤"å…¬å¸åç§°"æ ‡ç­¾æ–‡å­—
                            detail['company_name'] = full_text.replace('å…¬å¸åç§°', '').strip()
                    
                    # å…¬å¸ä»‹ç»ï¼ˆcompany-info-boxï¼‰
                    company_info_box = await company_section.query_selector('.company-info-box')
                    if company_info_box:
                        company_intro = await company_info_box.query_selector('.content')
                        if company_intro:
                            detail['company_intro'] = (await company_intro.text_content()).strip()
                    
                    # å·¥ä½œåœ°å€ï¼ˆcompany-addressï¼‰
                    address_box = await company_section.query_selector('.company-address')
                    if address_box:
                        address_text = await address_box.query_selector('.location-address')
                        if address_text:
                            detail['work_address'] = (await address_text.text_content()).strip()
                
                self.stats["success_requests"] += 1
                # logger.debug(f"âœ… è¯¦æƒ…è·å–æˆåŠŸ - {detail.get('job_title', 'N/A')}")
                
                return detail
            
            except asyncio.TimeoutError:
                logger.error(f"âŒ è¯¦æƒ…é¡µåŠ è½½è¶…æ—¶: {job_url}")
                self.stats["failed_requests"] += 1
                return None
            
            finally:
                await page.close()
                if should_close_context and temp_context:
                    await temp_context.close()
        
        except Exception as e:
            logger.error(f"âŒ è·å–èŒä½è¯¦æƒ…å¤±è´¥: {e}", exc_info=True)
            self.stats["failed_requests"] += 1
            return None
    
    async def search_and_get_details(
        self,
        keyword: str,
        city: str = "æ·±åœ³",
        page: int = 1,
        max_results: int = 10,
        max_concurrent: int = 5,  # å¢åŠ é»˜è®¤å¹¶å‘æ•°åˆ°5
        use_random_ua: bool = True  # é»˜è®¤å¯ç”¨éšæœºUA
    ) -> List[Dict]:
        """
        æœç´¢å¹¶è·å–è¯¦æƒ…
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            city: åŸå¸‚
            page: é¡µç 
            max_results: æœ€å¤šè·å–æ•°é‡
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            use_random_ua: æ˜¯å¦ä¸ºæ¯ä¸ªè¯¦æƒ…è¯·æ±‚ä½¿ç”¨éšæœºUser-Agent
        
        Returns:
            åŒ…å«å®Œæ•´ä¿¡æ¯çš„èŒä½åˆ—è¡¨
        """
        # 1. æœç´¢èŒä½ï¼ˆåªæœç´¢ä¸€æ¬¡ï¼‰
        jobs = await self.search_jobs(keyword, city, page)
        
        if not jobs:
            return []
        
        # é™åˆ¶æ•°é‡
        jobs = jobs[:max_results]
        
        # 2. è·å–è¯¦æƒ…ï¼ˆä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘ï¼‰
        logger.info(f"ğŸ“¥ å¼€å§‹è·å– {len(jobs)} ä¸ªèŒä½çš„è¯¦æƒ…ï¼ˆå¹¶å‘æ•°: {max_concurrent}, éšæœºUA: {use_random_ua}ï¼‰...")
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def fetch_detail_with_limit(job: Dict, index: int) -> Dict:
            """å¸¦é™æµçš„è¯¦æƒ…è·å–"""
            async with semaphore:
                job_url = job.get("job_url", "")
                if not job_url:
                    return job
                
                logger.info(f"  [{index}/{len(jobs)}] {job.get('title', 'N/A')}")
                
                detail = await self.get_job_detail(job_url, use_random_ua=use_random_ua)
                
                if detail:
                    job.update(detail)
                
                return job
        
        # å¹¶å‘è·å–è¯¦æƒ…
        jobs_with_details = await asyncio.gather(
            *[fetch_detail_with_limit(job, idx) for idx, job in enumerate(jobs, 1)],
            return_exceptions=True
        )
        
        # è¿‡æ»¤å¼‚å¸¸ç»“æœ
        valid_jobs = [
            job for job in jobs_with_details 
            if not isinstance(job, Exception)
        ]
        
        logger.info(f"âœ… æˆåŠŸè·å– {len(valid_jobs)}/{len(jobs)} ä¸ªèŒä½çš„å®Œæ•´ä¿¡æ¯")
        return valid_jobs
    
    def get_stats(self) -> Dict:
        """è·å–çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯"""
        success_rate = (
            self.stats["success_requests"] / self.stats["total_requests"] * 100
            if self.stats["total_requests"] > 0
            else 0
        )
        
        return {
            "total_requests": self.stats["total_requests"],
            "success_requests": self.stats["success_requests"],
            "failed_requests": self.stats["failed_requests"],
            "retried_requests": self.stats["retried_requests"],
            "success_rate": f"{success_rate:.2f}%",
            "unique_user_agents": len(self.stats["user_agents_used"]),
        }
