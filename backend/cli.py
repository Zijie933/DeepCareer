"""
DeepCareer å‘½ä»¤è¡Œå·¥å…·
ç”¨æ³•ï¼šdeepcareer <command> [options]
"""
import argparse
import asyncio
import sys
import json
from pathlib import Path

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨è·¯å¾„ä¸­
sys.path.insert(0, str(Path(__file__).parent.parent))


def crawl_command(args):
    """çˆ¬å–èŒä½å‘½ä»¤"""
    from backend.crawlers.boss_web_crawler_playwright import BossWebCrawlerPlaywright
    from backend.config import settings
    from backend.utils.logger import setup_logger
    import logging
    
    setup_logger()
    logger = logging.getLogger(__name__)
    
    async def run_crawl():
        # ä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå…¶æ¬¡ä½¿ç”¨ç¯å¢ƒå˜é‡
        cookie = args.cookie or settings.BOSS_COOKIE
        
        if not cookie:
            logger.error("âŒ æœªé…ç½®Cookieï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€è®¾ç½®ï¼š")
            logger.error("   1. ç¯å¢ƒå˜é‡: export BOSS_COOKIE='your_cookie'")
            logger.error("   2. .envæ–‡ä»¶: BOSS_COOKIE=your_cookie")
            logger.error("   3. å‘½ä»¤è¡Œå‚æ•°: --cookie 'your_cookie'")
            return []
        
        logger.info("=" * 70)
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å– {args.city} - {args.keyword} ï¼ˆç›®æ ‡ï¼š{args.count}ä¸ªï¼‰")
        logger.info("=" * 70)
        
        async with BossWebCrawlerPlaywright(
            min_delay=1.5,
            max_delay=3.0,
            headless=not args.visible,
            cookie_string=cookie,
            target_city=args.city
        ) as crawler:
            # æœç´¢èŒä½
            all_jobs = await crawler.search_jobs(
                keyword=args.keyword,
                city=args.city,
                page=1,
                auto_scroll=False
            )
            
            if not all_jobs:
                logger.warning("æœªæ‰¾åˆ°ä»»ä½•èŒä½")
                return []
            
            logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(all_jobs)} ä¸ªèŒä½")
            
            # è·å–è¯¦æƒ…
            jobs_to_fetch = all_jobs[:args.count]
            
            if args.detail:
                logger.info(f"ğŸ“¥ è·å– {len(jobs_to_fetch)} ä¸ªèŒä½è¯¦æƒ…ï¼ˆå¹¶å‘ï¼š{args.concurrent}ï¼‰...")
                
                semaphore = asyncio.Semaphore(args.concurrent)
                
                async def fetch_detail(job, idx):
                    async with semaphore:
                        url = job.get("job_url")
                        if url:
                            detail = await crawler.get_job_detail(url, use_random_ua=True)
                            if detail:
                                job.update(detail)
                        return job
                
                jobs = await asyncio.gather(
                    *[fetch_detail(j, i) for i, j in enumerate(jobs_to_fetch, 1)],
                    return_exceptions=True
                )
                jobs = [j for j in jobs if not isinstance(j, Exception)]
            else:
                jobs = jobs_to_fetch
            
            # ä¿å­˜ç»“æœ
            output = args.output or f"{args.city}_{args.keyword}_jobs.json"
            with open(output, "w", encoding="utf-8") as f:
                json.dump(jobs, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… æˆåŠŸçˆ¬å– {len(jobs)} ä¸ªèŒä½ï¼Œä¿å­˜è‡³: {output}")
            
            # æ‰“å°ç»Ÿè®¡
            stats = crawler.get_stats()
            logger.info(f"ğŸ“Š ç»Ÿè®¡: è¯·æ±‚{stats['total_requests']}æ¬¡, æˆåŠŸç‡{stats['success_rate']}")
            
            return jobs
    
    asyncio.run(run_crawl())


def serve_command(args):
    """å¯åŠ¨APIæœåŠ¡"""
    import uvicorn
    from backend.config import settings
    
    print(f"ğŸš€ å¯åŠ¨DeepCareer APIæœåŠ¡...")
    print(f"   åœ°å€: http://{args.host}:{args.port}")
    print(f"   æ–‡æ¡£: http://{args.host}:{args.port}/docs")
    
    uvicorn.run(
        "backend.main:app",
        host=args.host,
        port=args.port,
        reload=args.reload
    )


def cities_command(args):
    """åˆ—å‡ºæ”¯æŒçš„åŸå¸‚"""
    from backend.crawlers.boss_web_crawler_playwright import BossWebCrawlerPlaywright
    
    print("ğŸ“ æ”¯æŒçš„åŸå¸‚åˆ—è¡¨ï¼š")
    print("-" * 40)
    for city, code in BossWebCrawlerPlaywright.CITY_CODES.items():
        print(f"  {city}: {code}")


def version_command(args):
    """æ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯"""
    print("DeepCareer v1.0.0")
    print("æ™ºèƒ½èŒä½åŒ¹é…ç³»ç»Ÿ")


def frontend_command(args):
    """å¯åŠ¨å‰ç«¯å¼€å‘æœåŠ¡"""
    import subprocess
    import os
    
    frontend_dir = Path(__file__).parent.parent / "frontend"
    
    if not frontend_dir.exists():
        print("âŒ å‰ç«¯ç›®å½•ä¸å­˜åœ¨")
        return
    
    os.chdir(frontend_dir)
    
    if args.install:
        print("ğŸ“¦ å®‰è£…å‰ç«¯ä¾èµ–...")
        subprocess.run(["npm", "install"], check=True)
    
    print("ğŸš€ å¯åŠ¨å‰ç«¯å¼€å‘æœåŠ¡...")
    print("   åœ°å€: http://localhost:3000")
    subprocess.run(["npm", "run", "dev"])


def dev_command(args):
    """åŒæ—¶å¯åŠ¨å‰åç«¯å¼€å‘æœåŠ¡"""
    import subprocess
    import os
    import signal
    import time
    
    project_dir = Path(__file__).parent.parent
    frontend_dir = project_dir / "frontend"
    
    if not frontend_dir.exists():
        print("âŒ å‰ç«¯ç›®å½•ä¸å­˜åœ¨")
        return
    
    processes = []
    
    def cleanup(signum=None, frame=None):
        print("\nğŸ›‘ æ­£åœ¨å…³é—­æœåŠ¡...")
        for p in processes:
            try:
                p.terminate()
                p.wait(timeout=3)
            except:
                p.kill()
        print("âœ… æœåŠ¡å·²å…³é—­")
        sys.exit(0)
    
    signal.signal(signal.SIGINT, cleanup)
    signal.signal(signal.SIGTERM, cleanup)
    
    try:
        # å¯åŠ¨åç«¯
        print("ğŸš€ å¯åŠ¨åç«¯APIæœåŠ¡...")
        print(f"   åœ°å€: http://localhost:{args.backend_port}")
        print(f"   æ–‡æ¡£: http://localhost:{args.backend_port}/docs")
        
        backend_cmd = [
            sys.executable, "-m", "uvicorn", "backend.main:app",
            "--host", "0.0.0.0",
            "--port", str(args.backend_port)
        ]
        if args.reload:
            backend_cmd.append("--reload")
        
        backend_proc = subprocess.Popen(
            backend_cmd,
            cwd=project_dir,
            stdout=subprocess.PIPE if args.quiet else None,
            stderr=subprocess.STDOUT if args.quiet else None
        )
        processes.append(backend_proc)
        
        # ç­‰å¾…åç«¯å¯åŠ¨ï¼ˆæ£€æµ‹å¥åº·æ£€æŸ¥æ¥å£ï¼‰
        import urllib.request
        print("â³ ç­‰å¾…åç«¯å¯åŠ¨...")
        for i in range(30):  # æœ€å¤šç­‰30ç§’
            try:
                urllib.request.urlopen(f"http://localhost:{args.backend_port}/health", timeout=1)
                print("âœ… åç«¯å·²å°±ç»ª")
                break
            except:
                time.sleep(1)
        else:
            print("âš ï¸ åç«¯å¯åŠ¨è¶…æ—¶ï¼Œç»§ç»­å¯åŠ¨å‰ç«¯...")
        
        # å¯åŠ¨å‰ç«¯
        print("ğŸ¨ å¯åŠ¨å‰ç«¯å¼€å‘æœåŠ¡...")
        print(f"   åœ°å€: http://localhost:{args.frontend_port}")
        
        frontend_proc = subprocess.Popen(
            ["npm", "run", "dev", "--", "--port", str(args.frontend_port)],
            cwd=frontend_dir,
            stdout=subprocess.PIPE if args.quiet else None,
            stderr=subprocess.STDOUT if args.quiet else None
        )
        processes.append(frontend_proc)
        
        print("\n" + "=" * 50)
        print("âœ… DeepCareer å¼€å‘ç¯å¢ƒå·²å¯åŠ¨!")
        print(f"   å‰ç«¯: http://localhost:{args.frontend_port}")
        print(f"   åç«¯: http://localhost:{args.backend_port}")
        print(f"   æ–‡æ¡£: http://localhost:{args.backend_port}/docs")
        print("=" * 50)
        print("æŒ‰ Ctrl+C åœæ­¢æ‰€æœ‰æœåŠ¡\n")
        
        # ç­‰å¾…è¿›ç¨‹
        while True:
            for p in processes:
                if p.poll() is not None:
                    print(f"âš ï¸ æœåŠ¡å¼‚å¸¸é€€å‡º (code={p.returncode})")
                    cleanup()
            time.sleep(1)
            
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        cleanup()


def main():
    """CLIä¸»å…¥å£"""
    parser = argparse.ArgumentParser(
        prog="deepcareer",
        description="DeepCareer - æ™ºèƒ½èŒä½åŒ¹é…ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  deepcareer crawl --city æ·±åœ³ --keyword Python --count 10
  deepcareer serve --port 8001
  deepcareer cities
        """
    )
    
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")
    
    # ========== crawl å‘½ä»¤ ==========
    crawl_parser = subparsers.add_parser("crawl", help="çˆ¬å–BOSSç›´è˜èŒä½")
    crawl_parser.add_argument("--city", "-c", type=str, required=True,
                              help="åŸå¸‚åç§°ï¼ˆå¦‚ï¼šåŒ—äº¬ã€ä¸Šæµ·ã€æ·±åœ³ï¼‰")
    crawl_parser.add_argument("--keyword", "-k", type=str, required=True,
                              help="æœç´¢å…³é”®è¯ï¼ˆå¦‚ï¼šPythonã€Javaï¼‰")
    crawl_parser.add_argument("--count", "-n", type=int, default=10,
                              help="çˆ¬å–æ•°é‡ï¼ˆé»˜è®¤ï¼š10ï¼‰")
    crawl_parser.add_argument("--output", "-o", type=str, default=None,
                              help="è¾“å‡ºæ–‡ä»¶å")
    crawl_parser.add_argument("--concurrent", type=int, default=5,
                              help="å¹¶å‘æ•°ï¼ˆé»˜è®¤ï¼š5ï¼‰")
    crawl_parser.add_argument("--cookie", type=str, default=None,
                              help="BOSSç›´è˜Cookie")
    crawl_parser.add_argument("--no-detail", dest="detail", action="store_false",
                              help="ä¸è·å–è¯¦æƒ…é¡µ")
    crawl_parser.add_argument("--visible", action="store_true",
                              help="æ˜¾ç¤ºæµè§ˆå™¨çª—å£")
    crawl_parser.set_defaults(func=crawl_command, detail=True)
    
    # ========== serve å‘½ä»¤ ==========
    serve_parser = subparsers.add_parser("serve", help="å¯åŠ¨APIæœåŠ¡")
    serve_parser.add_argument("--host", "-H", type=str, default="0.0.0.0",
                              help="ç›‘å¬åœ°å€ï¼ˆé»˜è®¤ï¼š0.0.0.0ï¼‰")
    serve_parser.add_argument("--port", "-p", type=int, default=8001,
                              help="ç›‘å¬ç«¯å£ï¼ˆé»˜è®¤ï¼š8001ï¼‰")
    serve_parser.add_argument("--reload", "-r", action="store_true",
                              help="å¼€å‘æ¨¡å¼ï¼ˆè‡ªåŠ¨é‡è½½ï¼‰")
    serve_parser.set_defaults(func=serve_command)
    
    # ========== cities å‘½ä»¤ ==========
    cities_parser = subparsers.add_parser("cities", help="åˆ—å‡ºæ”¯æŒçš„åŸå¸‚")
    cities_parser.set_defaults(func=cities_command)
    
    # ========== version å‘½ä»¤ ==========
    version_parser = subparsers.add_parser("version", help="æ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯")
    version_parser.set_defaults(func=version_command)
    
    # ========== frontend å‘½ä»¤ ==========
    frontend_parser = subparsers.add_parser("frontend", help="å¯åŠ¨å‰ç«¯å¼€å‘æœåŠ¡")
    frontend_parser.add_argument("--install", "-i", action="store_true",
                                  help="å…ˆå®‰è£…ä¾èµ–")
    frontend_parser.set_defaults(func=frontend_command)
    
    # ========== dev å‘½ä»¤ ==========
    dev_parser = subparsers.add_parser("dev", help="åŒæ—¶å¯åŠ¨å‰åç«¯å¼€å‘æœåŠ¡")
    dev_parser.add_argument("--frontend-port", "-f", type=int, default=3000,
                            help="å‰ç«¯ç«¯å£ï¼ˆé»˜è®¤ï¼š3000ï¼‰")
    dev_parser.add_argument("--backend-port", "-b", type=int, default=8001,
                            help="åç«¯ç«¯å£ï¼ˆé»˜è®¤ï¼š8001ï¼‰")
    dev_parser.add_argument("--reload", "-r", action="store_true",
                            help="åç«¯è‡ªåŠ¨é‡è½½")
    dev_parser.add_argument("--quiet", "-q", action="store_true",
                            help="é™é»˜æ¨¡å¼ï¼ˆéšè—å­è¿›ç¨‹è¾“å‡ºï¼‰")
    dev_parser.set_defaults(func=dev_command)
    
    # è§£æå‚æ•°
    args = parser.parse_args()
    
    if args.command is None:
        parser.print_help()
        sys.exit(0)
    
    # æ‰§è¡Œå‘½ä»¤
    args.func(args)


if __name__ == "__main__":
    main()
