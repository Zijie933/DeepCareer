"""
æ™ºèƒ½åŒ¹é…API - é€‰æ‹©ç®€å†è‡ªåŠ¨åŒ¹é…å²—ä½
æµç¨‹ï¼š
1. ç”¨æˆ·é€‰æ‹©å·²ä¸Šä¼ çš„ç®€å†
2. ä»ç®€å†æå–æœç´¢å…³é”®è¯ï¼ˆèŒä½ã€æŠ€èƒ½ç­‰ï¼‰
3. å…ˆä»æ•°æ®åº“åŒ¹é…å²—ä½ï¼ˆç«‹å³è¿”å›ç¬¬ä¸€æ‰¹ç»“æœï¼‰
4. å¦‚æœä¸è¶³10ä¸ªï¼Œè§¦å‘çˆ¬è™«æœç´¢ï¼ˆæ”¯æŒå¤šå…³é”®è¯ï¼‰
5. çˆ¬è™«ç»“æœä¿å­˜/æ›´æ–°åˆ°æ•°æ®åº“ï¼ˆupsertï¼‰
6. æµå¼è¿”å›åŒ¹é…ç»“æœ
"""
from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from fastapi.responses import StreamingResponse
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, update
from typing import Optional, List
from pydantic import BaseModel
import asyncio
import json

from backend.database.connection import get_db, async_session_factory
from backend.models.resume_v2 import ResumeV2
from backend.models.job_v2 import JobV2
from backend.models.match_record import MatchRecord
from backend.services.matcher_service import MatcherService
from backend.services.extractor_service import ExtractorService
from backend.crawlers.boss_web_crawler_playwright import BossWebCrawlerPlaywright
from backend.utils.local_embedding import LocalEmbeddingService
from backend.utils.logger import logger
from backend.config import settings

router = APIRouter(prefix="/api/v2/smart-match", tags=["æ™ºèƒ½åŒ¹é…"])

matcher = MatcherService()
extractor = ExtractorService()
embedding_service = LocalEmbeddingService()


class SmartMatchRequest(BaseModel):
    """æ™ºèƒ½åŒ¹é…è¯·æ±‚"""
    resume_id: int  # ç®€å†ID
    min_jobs: int = 10  # æœ€å°‘åŒ¹é…å²—ä½æ•°ï¼ˆæŒ‡60%ä»¥ä¸Šçš„ï¼‰
    max_jobs: int = 20  # æœ€å¤šè¿”å›å²—ä½æ•°
    city: Optional[str] = None  # æŒ‡å®šåŸå¸‚ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç®€å†æå–ï¼‰
    extra_keywords: Optional[List[str]] = None  # é¢å¤–æœç´¢å…³é”®è¯
    enable_crawler: bool = True  # æ˜¯å¦å¯ç”¨çˆ¬è™«è¡¥å……
    qualified_threshold: float = 60.0  # åˆæ ¼åŒ¹é…åˆ†æ•°é˜ˆå€¼ï¼ˆé»˜è®¤60%ï¼‰
    min_display_score: float = 30.0  # æœ€ä½å±•ç¤ºåˆ†æ•°ï¼ˆä½äºæ­¤åˆ†æ•°ä¸å±•ç¤ºï¼‰


class SmartMatchResponse(BaseModel):
    """æ™ºèƒ½åŒ¹é…å“åº”"""
    resume_id: int
    resume_name: str
    search_keywords: List[str]  # ä½¿ç”¨çš„æœç´¢å…³é”®è¯
    target_city: str  # ç›®æ ‡åŸå¸‚
    total_matched: int  # æ€»åŒ¹é…æ•°
    qualified_count: int  # åˆæ ¼æ•°é‡ï¼ˆ>=60%ï¼‰
    from_database: int  # æ¥è‡ªæ•°æ®åº“çš„æ•°é‡
    from_crawler: int  # æ¥è‡ªçˆ¬è™«çš„æ•°é‡
    matches: List[dict]  # åŒ¹é…ç»“æœï¼ˆæŒ‰åˆ†æ•°æ’åºï¼ŒåŒ…å«åˆæ ¼å’Œä¸åˆæ ¼ï¼‰


def extract_search_keywords(resume_data: dict) -> List[str]:
    """
    ä»ç®€å†æ•°æ®æå–æœç´¢å…³é”®è¯
    
    ä¼˜å…ˆçº§ï¼š
    1. å½“å‰èŒä½
    2. æ±‚èŒæ„å‘èŒä½
    3. æ ¸å¿ƒæŠ€èƒ½
    """
    keywords = []
    
    # 1. å½“å‰èŒä½
    current_position = resume_data.get('current_position')
    if current_position:
        # æ¸…ç†èŒä½åç§°
        position = current_position.replace('é«˜çº§', '').replace('èµ„æ·±', '').replace('åˆçº§', '').strip()
        if position and len(position) >= 2:
            keywords.append(position)
    
    # 2. æ±‚èŒæ„å‘
    job_intention = resume_data.get('job_intention', {})
    if job_intention:
        positions = job_intention.get('positions', [])
        for pos in positions[:2]:  # æœ€å¤šå–2ä¸ª
            if pos and pos not in keywords:
                keywords.append(pos)
    
    # 3. æ ¸å¿ƒæŠ€èƒ½ï¼ˆé€‰æ‹©çƒ­é—¨æŠ€èƒ½ï¼‰
    skills = resume_data.get('skills', {})
    hot_skills = []
    
    if isinstance(skills, dict):
        # åˆ†ç±»æŠ€èƒ½
        for category in ['programming_languages', 'frameworks']:
            category_skills = skills.get(category, [])
            hot_skills.extend(category_skills[:2])
    elif isinstance(skills, list):
        hot_skills = skills[:3]
    
    # ç»„åˆæŠ€èƒ½å…³é”®è¯
    for skill in hot_skills[:3]:
        if skill and skill not in keywords:
            # æŠ€èƒ½é€šå¸¸éœ€è¦ç»„åˆæœç´¢ï¼Œå¦‚ "Pythonå¼€å‘"
            keywords.append(skill)
    
    # 4. å¦‚æœè¿˜æ²¡æœ‰å…³é”®è¯ï¼Œä½¿ç”¨é€šç”¨å…³é”®è¯
    if not keywords:
        keywords = ['å¼€å‘å·¥ç¨‹å¸ˆ', 'Python', 'Java']
    
    return keywords[:5]  # æœ€å¤š5ä¸ªå…³é”®è¯


async def crawl_jobs_for_keywords(
    keywords: List[str],
    city: str,
    max_per_keyword: int,
    db: AsyncSession
) -> List[dict]:
    """
    æ ¹æ®å¤šä¸ªå…³é”®è¯çˆ¬å–èŒä½ï¼ˆå¿«é€Ÿç‰ˆï¼Œä¸æ»šåŠ¨ï¼‰
    
    Args:
        keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
        city: åŸå¸‚
        max_per_keyword: æ¯ä¸ªå…³é”®è¯æœ€å¤šçˆ¬å–æ•°é‡
        db: æ•°æ®åº“ä¼šè¯
    
    Returns:
        çˆ¬å–å¹¶ä¿å­˜çš„èŒä½åˆ—è¡¨
    """
    if not settings.BOSS_COOKIE:
        logger.warning("æœªé…ç½®BOSS_COOKIEï¼Œè·³è¿‡çˆ¬è™«")
        return []
    
    logger.info(f"ğŸª BOSS_COOKIEå·²é…ç½®ï¼Œé•¿åº¦: {len(settings.BOSS_COOKIE)} å­—ç¬¦")
    logger.info(f"ğŸ”§ çˆ¬è™«å‚æ•°: keywords={keywords}, city={city}, max_per_keyword={max_per_keyword}")
    
    all_jobs = []
    seen_job_ids = set()
    
    async with BossWebCrawlerPlaywright(
        min_delay=1.0,
        max_delay=2.0,
        headless=True,
        cookie_string=settings.BOSS_COOKIE,
        target_city=city
    ) as crawler:
        for keyword in keywords:
            try:
                logger.info(f"ğŸ” çˆ¬å–å…³é”®è¯: {keyword}, åŸå¸‚: {city}")
                
                # å¿«é€Ÿæœç´¢èŒä½ï¼ˆä¸æ»šåŠ¨ï¼Œåªå–é¦–å±ï¼Œé€Ÿåº¦ä¼˜å…ˆï¼‰
                jobs = await crawler.search_jobs(
                    keyword=keyword,
                    city=city,
                    page=1,
                    auto_scroll=False,  # ä¸æ»šåŠ¨ï¼Œå¿«é€Ÿè¿”å›
                    max_scroll=0
                )
                
                if not jobs:
                    logger.warning(f"å…³é”®è¯ '{keyword}' æœªæ‰¾åˆ°èŒä½")
                    continue
                
                # é™åˆ¶æ•°é‡å¹¶å»é‡
                for job in jobs[:max_per_keyword]:
                    job_id = job.get('job_id', '')
                    if job_id and job_id not in seen_job_ids:
                        seen_job_ids.add(job_id)
                        job['search_keyword'] = keyword
                        all_jobs.append(job)
                
                logger.info(f"âœ… å…³é”®è¯ '{keyword}' æ‰¾åˆ° {len(jobs)} ä¸ªèŒä½")
                
            except Exception as e:
                logger.error(f"âŒ çˆ¬å–å…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
                continue
    
    # è·å–è¯¦æƒ…å¹¶ä¿å­˜åˆ°æ•°æ®åº“
    saved_jobs = []
    
    for job in all_jobs:
        try:
            job_id = job.get('job_id', '')
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            result = await db.execute(
                select(JobV2).where(JobV2.external_id == job_id)
            )
            existing = result.scalar_one_or_none()
            
            # æ„å»ºæè¿°
            full_desc = job.get('job_description', '')
            if not full_desc:
                full_desc = f"{job.get('title', '')}\nå…¬å¸ï¼š{job.get('company', '')}\nè–ªèµ„ï¼š{job.get('salary', '')}"
            
            # æå–ç»“æ„åŒ–æ•°æ®
            structured_data, confidence, method = extractor.extract_job(
                text=full_desc,
                use_llm=False
            )
            
            # è¡¥å……ä¿¡æ¯
            structured_data['title'] = job.get('title', '')
            structured_data['company'] = job.get('company', '')
            structured_data['salary_range'] = job.get('salary', '')
            structured_data['job_keywords'] = job.get('job_keywords', [])
            
            # ç”Ÿæˆå‘é‡
            try:
                embedding = embedding_service.create_embedding(full_desc[:1000])
            except:
                embedding = None
            
            if existing:
                # æ›´æ–°å·²å­˜åœ¨çš„èŒä½
                await db.execute(
                    update(JobV2).where(JobV2.id == existing.id).values(
                        title=job.get('title', existing.title),
                        company_name=job.get('company_name', job.get('company', existing.company_name)),
                        salary_text=job.get('salary', existing.salary_text),
                        full_description=full_desc if full_desc else existing.full_description,
                        structured_data=structured_data,
                        description_embedding=embedding if embedding else existing.description_embedding,
                        is_active=True
                    )
                )
                await db.commit()
                
                # åˆ·æ–°è·å–æœ€æ–°æ•°æ®
                await db.refresh(existing)
                saved_jobs.append({
                    'id': existing.id,
                    'title': existing.title,
                    'company_name': existing.company_name,
                    'city': existing.city,
                    'salary_text': existing.salary_text,
                    'job_url': existing.job_url,
                    'from_crawler': True,
                    'updated': True
                })
                logger.info(f"ğŸ”„ æ›´æ–°èŒä½: {job.get('title')} (ID={existing.id})")
            else:
                # åˆ›å»ºæ–°èŒä½
                job_record = JobV2(
                    external_id=job_id,
                    platform="boss",
                    job_url=job.get('job_url', ''),
                    title=job.get('title', ''),
                    company_name=job.get('company_name', job.get('company', '')),
                    city=job.get('work_city', city),
                    salary_text=job.get('salary', ''),
                    experience_required=job.get('experience', ''),
                    education_required=job.get('education', ''),
                    full_description=full_desc,
                    structured_data=structured_data,
                    extraction_method=method,
                    extraction_confidence=confidence,
                    description_embedding=embedding,
                    is_active=True
                )
                
                db.add(job_record)
                await db.commit()
                await db.refresh(job_record)
                
                saved_jobs.append({
                    'id': job_record.id,
                    'title': job_record.title,
                    'company_name': job_record.company_name,
                    'city': job_record.city,
                    'salary_text': job_record.salary_text,
                    'job_url': job_record.job_url,
                    'from_crawler': True,
                    'updated': False
                })
                logger.info(f"âœ… æ–°å¢èŒä½: {job.get('title')} (ID={job_record.id})")
        
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜èŒä½å¤±è´¥: {e}")
            continue
    
    return saved_jobs


@router.post("/stream")
async def smart_match_stream(
    request: SmartMatchRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    æµå¼æ™ºèƒ½åŒ¹é…å²—ä½ï¼ˆSSEï¼‰
    
    æµç¨‹ï¼š
    1. ç«‹å³è¿”å›æ•°æ®åº“åŒ¹é…ç»“æœï¼ˆç¬¬ä¸€æ‰¹ï¼‰
    2. å‰ç«¯æ˜¾ç¤º"æ­£åœ¨è·å–æ›´å¤šèŒä½..."
    3. åå°çˆ¬è™«æœç´¢ï¼Œå®æ—¶æ¨é€æ–°ç»“æœ
    4. å®Œæˆåå‘é€ç»“æŸä¿¡å·
    
    SSE äº‹ä»¶ç±»å‹ï¼š
    - db_matches: æ•°æ®åº“åŒ¹é…ç»“æœ
    - crawling: çˆ¬è™«è¿›åº¦
    - crawler_matches: çˆ¬è™«åŒ¹é…ç»“æœ
    - complete: å®Œæˆ
    - error: é”™è¯¯
    """
    
    async def generate_matches():
        try:
            # 1. è·å–ç®€å†
            result = await db.execute(select(ResumeV2).where(ResumeV2.id == request.resume_id))
            resume = result.scalar_one_or_none()
            if not resume:
                yield f"data: {json.dumps({'type': 'error', 'message': 'ç®€å†ä¸å­˜åœ¨'})}\n\n"
                return
            
            resume_data = resume.structured_data or {}
            resume_name = resume_data.get('name', 'æœªçŸ¥')
            
            # 2. æå–æœç´¢å…³é”®è¯
            keywords = extract_search_keywords(resume_data)
            if request.extra_keywords:
                keywords.extend(request.extra_keywords)
            keywords = list(set(keywords))[:5]
            
            # 3. ç¡®å®šåŸå¸‚
            city = request.city
            if not city:
                city = resume_data.get('location')
                if not city:
                    job_intention = resume_data.get('job_intention', {})
                    cities = job_intention.get('cities', [])
                    city = cities[0] if cities else 'æ·±åœ³'
            city = city.replace('å¸‚', '').strip()
            
            logger.info(f"ğŸš€ æµå¼åŒ¹é…å¼€å§‹: resume_id={request.resume_id}, city={city}")
            
            # 4. ä»æ•°æ®åº“æŸ¥è¯¢åŒåŸå¸‚çš„èŒä½
            db_jobs_result = await db.execute(
                select(JobV2).where(
                    JobV2.is_active == True,
                    JobV2.city.ilike(f"%{city}%")
                ).limit(200)
            )
            db_jobs = db_jobs_result.scalars().all()
            
            # 5. è®¡ç®—åŒ¹é…åˆ†æ•°
            qualified_matches = []
            unqualified_matches = []
            
            for job in db_jobs:
                try:
                    score, details = matcher.fast_match(
                        resume_data=resume_data,
                        job_data=job.structured_data or {},
                        resume_embedding=resume.text_embedding,
                        job_embedding=job.description_embedding
                    )
                    
                    if score < request.min_display_score:
                        continue
                    
                    match_item = {
                        'job_id': job.id,
                        'title': job.title,
                        'company_name': job.company_name,
                        'city': job.city,
                        'salary_text': job.salary_text,
                        'job_url': job.job_url,
                        'experience_required': job.experience_required,
                        'education_required': job.education_required,
                        'match_score': round(score, 2),
                        'match_details': details,
                        'is_qualified': score >= request.qualified_threshold,
                        'from_database': True,
                        'from_crawler': False
                    }
                    
                    if score >= request.qualified_threshold:
                        qualified_matches.append(match_item)
                    else:
                        unqualified_matches.append(match_item)
                except Exception as e:
                    logger.error(f"åŒ¹é…èŒä½ {job.id} å¤±è´¥: {e}")
                    continue
            
            # æ’åº
            qualified_matches.sort(key=lambda x: x['match_score'], reverse=True)
            unqualified_matches.sort(key=lambda x: x['match_score'], reverse=True)
            
            all_db_matches = qualified_matches + unqualified_matches
            qualified_count = len(qualified_matches)
            
            logger.info(f"ğŸ“Š æ•°æ®åº“åŒ¹é…å®Œæˆ: åˆæ ¼{qualified_count}ä¸ª, ä¸åˆæ ¼{len(unqualified_matches)}ä¸ª")
            
            # 6. ç«‹å³å‘é€æ•°æ®åº“åŒ¹é…ç»“æœ
            yield f"data: {json.dumps({'type': 'db_matches', 'data': {'resume_id': resume.id, 'resume_name': resume_name, 'target_city': city, 'search_keywords': keywords, 'matches': all_db_matches, 'qualified_count': qualified_count, 'from_database': len(all_db_matches), 'from_crawler': 0, 'need_crawler': request.enable_crawler and qualified_count < request.min_jobs}}, ensure_ascii=False)}\n\n"
            
            # 7. å¦‚æœåˆæ ¼æ•°é‡ä¸è¶³ï¼Œå¯åŠ¨çˆ¬è™«
            if request.enable_crawler and qualified_count < request.min_jobs:
                needed = request.min_jobs - qualified_count
                
                yield f"data: {json.dumps({'type': 'crawling', 'message': f'åˆæ ¼èŒä½ä¸è¶³ï¼Œæ­£åœ¨æœç´¢æ›´å¤šèŒä½...', 'needed': needed}, ensure_ascii=False)}\n\n"
                
                # çˆ¬è™«æœç´¢ï¼ˆä½¿ç”¨æ–°çš„æ•°æ®åº“ä¼šè¯ï¼‰
                crawler_matches = []
                existing_job_ids = [m['job_id'] for m in all_db_matches]
                
                try:
                    async with async_session_factory() as crawler_db:
                        crawled_jobs = await crawl_jobs_for_keywords(
                            keywords=keywords,
                            city=city,
                            max_per_keyword=max(5, needed // len(keywords) + 1),
                            db=crawler_db
                        )
                        
                        # å¯¹çˆ¬å–çš„èŒä½è®¡ç®—åŒ¹é…åˆ†æ•°
                        for crawled in crawled_jobs:
                            if crawled['id'] in existing_job_ids:
                                continue
                            
                            job_result = await crawler_db.execute(
                                select(JobV2).where(JobV2.id == crawled['id'])
                            )
                            job = job_result.scalar_one_or_none()
                            
                            if job:
                                try:
                                    score, details = matcher.fast_match(
                                        resume_data=resume_data,
                                        job_data=job.structured_data or {},
                                        resume_embedding=resume.text_embedding,
                                        job_embedding=job.description_embedding
                                    )
                                    
                                    if score < request.min_display_score:
                                        continue
                                    
                                    match_item = {
                                        'job_id': job.id,
                                        'title': job.title,
                                        'company_name': job.company_name,
                                        'city': job.city,
                                        'salary_text': job.salary_text,
                                        'job_url': job.job_url,
                                        'experience_required': job.experience_required,
                                        'education_required': job.education_required,
                                        'match_score': round(score, 2),
                                        'match_details': details,
                                        'is_qualified': score >= request.qualified_threshold,
                                        'from_database': False,
                                        'from_crawler': True
                                    }
                                    
                                    crawler_matches.append(match_item)
                                    existing_job_ids.append(job.id)
                                    
                                    # æ¯æ‰¾åˆ°ä¸€ä¸ªæ–°èŒä½å°±æ¨é€
                                    yield f"data: {json.dumps({'type': 'crawler_match', 'data': match_item}, ensure_ascii=False)}\n\n"
                                    
                                except Exception as e:
                                    logger.error(f"åŒ¹é…çˆ¬å–èŒä½å¤±è´¥: {e}")
                                    continue
                    
                    logger.info(f"ğŸ•·ï¸ çˆ¬è™«å®Œæˆ: æ–°å¢{len(crawler_matches)}ä¸ªåŒ¹é…")
                    
                except Exception as e:
                    logger.error(f"âŒ çˆ¬è™«å¤±è´¥: {e}")
                    yield f"data: {json.dumps({'type': 'error', 'message': f'çˆ¬è™«æœç´¢å¤±è´¥: {str(e)}'}, ensure_ascii=False)}\n\n"
            
            # 8. å‘é€å®Œæˆä¿¡å·
            final_qualified = qualified_count + sum(1 for m in crawler_matches if m.get('is_qualified', False)) if 'crawler_matches' in dir() else qualified_count
            yield f"data: {json.dumps({'type': 'complete', 'message': 'åŒ¹é…å®Œæˆ', 'total_qualified': final_qualified}, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            logger.error(f"æµå¼åŒ¹é…é”™è¯¯: {e}")
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
    
    return StreamingResponse(
        generate_matches(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


@router.post("/", response_model=SmartMatchResponse)
async def smart_match(
    request: SmartMatchRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    æ™ºèƒ½åŒ¹é…å²—ä½ï¼ˆéæµå¼ï¼Œç­‰å¾…å…¨éƒ¨å®Œæˆï¼‰
    
    æµç¨‹ï¼š
    1. è·å–ç®€å†å¹¶æå–æœç´¢å…³é”®è¯
    2. ç¡®å®šç›®æ ‡åŸå¸‚
    3. ä»æ•°æ®åº“æŸ¥è¯¢ã€åŒåŸå¸‚ã€‘çš„èŒä½ï¼ˆåœ°åŒºç­›é€‰ä¼˜å…ˆï¼‰
    4. è®¡ç®—åŒ¹é…åˆ†æ•°ï¼ŒåŒºåˆ†åˆæ ¼ï¼ˆ>=60%ï¼‰å’Œä¸åˆæ ¼
    5. å¦‚æœåˆæ ¼æ•°é‡ä¸è¶³min_jobsï¼Œè§¦å‘çˆ¬è™«æœç´¢
    6. è¿”å›åŒ¹é…ç»“æœï¼ˆåˆæ ¼çš„åœ¨å‰ï¼Œä¸åˆæ ¼çš„åœ¨åï¼‰
    
    Args:
        request: æ™ºèƒ½åŒ¹é…è¯·æ±‚
    
    Returns:
        åŒ¹é…ç»“æœ
    """
    logger.info(f"ğŸš€ å¼€å§‹æ™ºèƒ½åŒ¹é…: resume_id={request.resume_id}")
    
    # 1. è·å–ç®€å†
    result = await db.execute(select(ResumeV2).where(ResumeV2.id == request.resume_id))
    resume = result.scalar_one_or_none()
    if not resume:
        raise HTTPException(status_code=404, detail="ç®€å†ä¸å­˜åœ¨")
    
    resume_data = resume.structured_data or {}
    resume_name = resume_data.get('name', 'æœªçŸ¥')
    
    # 2. æå–æœç´¢å…³é”®è¯
    keywords = extract_search_keywords(resume_data)
    if request.extra_keywords:
        keywords.extend(request.extra_keywords)
    keywords = list(set(keywords))[:5]  # å»é‡ï¼Œæœ€å¤š5ä¸ª
    
    logger.info(f"ğŸ“ æœç´¢å…³é”®è¯: {keywords}")
    
    # 3. ç¡®å®šåŸå¸‚ï¼ˆåœ°åŒºç­›é€‰ä¼˜å…ˆï¼‰
    city = request.city
    if not city:
        # ä»ç®€å†æå–
        city = resume_data.get('location')
        if not city:
            job_intention = resume_data.get('job_intention', {})
            cities = job_intention.get('cities', [])
            city = cities[0] if cities else 'æ·±åœ³'
    
    # æ ‡å‡†åŒ–åŸå¸‚åç§°ï¼ˆå»é™¤"å¸‚"åç¼€ï¼‰
    city = city.replace('å¸‚', '').strip()
    
    logger.info(f"ğŸ“ ç›®æ ‡åŸå¸‚: {city}ï¼ˆåœ°åŒºç­›é€‰ä¼˜å…ˆï¼‰")
    
    # 4. ä»æ•°æ®åº“æŸ¥è¯¢ã€åŒåŸå¸‚ã€‘çš„èŒä½ï¼ˆåœ°åŒºç­›é€‰ä¼˜å…ˆï¼ï¼‰
    db_jobs_result = await db.execute(
        select(JobV2).where(
            JobV2.is_active == True,
            # åœ°åŒºç­›é€‰ï¼šåŸå¸‚å¿…é¡»åŒ¹é…
            JobV2.city.ilike(f"%{city}%")
        ).limit(200)
    )
    db_jobs = db_jobs_result.scalars().all()
    
    logger.info(f"ğŸ“Š æ•°æ®åº“ä¸­ã€{city}ã€‘æœ‰ {len(db_jobs)} ä¸ªæ´»è·ƒèŒä½")
    
    # 5. è®¡ç®—åŒ¹é…åˆ†æ•°
    qualified_matches = []  # åˆæ ¼çš„ï¼ˆ>=60%ï¼‰
    unqualified_matches = []  # ä¸åˆæ ¼çš„ï¼ˆ<60%ä½†>=min_display_scoreï¼‰
    
    for job in db_jobs:
        try:
            score, details = matcher.fast_match(
                resume_data=resume_data,
                job_data=job.structured_data or {},
                resume_embedding=resume.text_embedding,
                job_embedding=job.description_embedding
            )
            
            # ä½äºæœ€ä½å±•ç¤ºåˆ†æ•°çš„ç›´æ¥è·³è¿‡
            if score < request.min_display_score:
                continue
            
            match_item = {
                'job_id': job.id,
                'title': job.title,
                'company_name': job.company_name,
                'city': job.city,
                'salary_text': job.salary_text,
                'job_url': job.job_url,
                'experience_required': job.experience_required,
                'education_required': job.education_required,
                'match_score': round(score, 2),
                'match_details': details,
                'is_qualified': score >= request.qualified_threshold,  # æ˜¯å¦åˆæ ¼
                'from_database': True,
                'from_crawler': False
            }
            
            if score >= request.qualified_threshold:
                qualified_matches.append(match_item)
            else:
                unqualified_matches.append(match_item)
                
        except Exception as e:
            logger.error(f"åŒ¹é…èŒä½ {job.id} å¤±è´¥: {e}")
            continue
    
    # æ’åº
    qualified_matches.sort(key=lambda x: x['match_score'], reverse=True)
    unqualified_matches.sort(key=lambda x: x['match_score'], reverse=True)
    
    from_database_count = len(qualified_matches) + len(unqualified_matches)
    qualified_count = len(qualified_matches)
    
    logger.info(f"âœ… æ•°æ®åº“åŒ¹é…: åˆæ ¼({qualified_count}ä¸ª, >={request.qualified_threshold}%), ä¸åˆæ ¼({len(unqualified_matches)}ä¸ª)")
    
    # 6. å¦‚æœåˆæ ¼æ•°é‡ä¸è¶³ï¼Œè§¦å‘çˆ¬è™«
    from_crawler_count = 0
    if request.enable_crawler and qualified_count < request.min_jobs:
        needed = request.min_jobs - qualified_count
        max_per_keyword = max(5, needed // len(keywords) + 1)
        
        logger.info(f"ğŸ•·ï¸ åˆæ ¼èŒä½ä¸è¶³({qualified_count}<{request.min_jobs})ï¼Œå¯åŠ¨çˆ¬è™«è¡¥å……ï¼ˆéœ€è¦ {needed} ä¸ªï¼‰")
        
        try:
            crawled_jobs = await crawl_jobs_for_keywords(
                keywords=keywords,
                city=city,
                max_per_keyword=max_per_keyword,
                db=db
            )
            
            # å¯¹çˆ¬å–çš„èŒä½è®¡ç®—åŒ¹é…åˆ†æ•°
            for crawled in crawled_jobs:
                # æ£€æŸ¥æ˜¯å¦å·²åœ¨åŒ¹é…åˆ—è¡¨ä¸­
                all_job_ids = [m['job_id'] for m in qualified_matches + unqualified_matches]
                if crawled['id'] in all_job_ids:
                    continue
                
                # è·å–å®Œæ•´èŒä½ä¿¡æ¯
                job_result = await db.execute(
                    select(JobV2).where(JobV2.id == crawled['id'])
                )
                job = job_result.scalar_one_or_none()
                
                if job:
                    try:
                        score, details = matcher.fast_match(
                            resume_data=resume_data,
                            job_data=job.structured_data or {},
                            resume_embedding=resume.text_embedding,
                            job_embedding=job.description_embedding
                        )
                        
                        if score < request.min_display_score:
                            continue
                        
                        match_item = {
                            'job_id': job.id,
                            'title': job.title,
                            'company_name': job.company_name,
                            'city': job.city,
                            'salary_text': job.salary_text,
                            'job_url': job.job_url,
                            'experience_required': job.experience_required,
                            'education_required': job.education_required,
                            'match_score': round(score, 2),
                            'match_details': details,
                            'is_qualified': score >= request.qualified_threshold,
                            'from_database': False,
                            'from_crawler': True
                        }
                        
                        if score >= request.qualified_threshold:
                            qualified_matches.append(match_item)
                        else:
                            unqualified_matches.append(match_item)
                        
                        from_crawler_count += 1
                    except Exception as e:
                        logger.error(f"åŒ¹é…çˆ¬å–èŒä½å¤±è´¥: {e}")
                        continue
            
            # é‡æ–°æ’åº
            qualified_matches.sort(key=lambda x: x['match_score'], reverse=True)
            unqualified_matches.sort(key=lambda x: x['match_score'], reverse=True)
            
            logger.info(f"ğŸ•·ï¸ çˆ¬è™«è¡¥å……äº† {from_crawler_count} ä¸ªåŒ¹é…èŒä½")
        
        except Exception as e:
            logger.error(f"âŒ çˆ¬è™«å¤±è´¥: {e}")
    
    # 7. åˆå¹¶ç»“æœï¼šåˆæ ¼çš„åœ¨å‰ï¼Œä¸åˆæ ¼çš„åœ¨å
    all_matches = qualified_matches + unqualified_matches
    
    # é™åˆ¶æ€»æ•°é‡
    final_matches = all_matches[:request.max_jobs]
    final_qualified_count = sum(1 for m in final_matches if m['is_qualified'])
    
    # 8. ä¿å­˜åŒ¹é…è®°å½•
    for match in final_matches:
        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰è®°å½•
            existing = await db.execute(
                select(MatchRecord).where(
                    MatchRecord.resume_id == resume.id,
                    MatchRecord.job_id == match['job_id'],
                    MatchRecord.match_method == 'fast'
                )
            )
            if not existing.scalar_one_or_none():
                record = MatchRecord(
                    resume_id=resume.id,
                    job_id=match['job_id'],
                    match_method='fast',
                    fast_score=match['match_score'],
                    fast_details=match['match_details']
                )
                db.add(record)
        except:
            pass
    
    await db.commit()
    
    logger.info(f"ğŸ‰ æ™ºèƒ½åŒ¹é…å®Œæˆ: å…± {len(final_matches)} ä¸ªç»“æœ, åˆæ ¼ {final_qualified_count} ä¸ª")
    
    return SmartMatchResponse(
        resume_id=resume.id,
        resume_name=resume_name,
        search_keywords=keywords,
        target_city=city,
        total_matched=len(final_matches),
        qualified_count=final_qualified_count,
        from_database=from_database_count,
        from_crawler=from_crawler_count,
        matches=final_matches
    )


@router.get("/resumes")
async def list_resumes_for_match(
    db: AsyncSession = Depends(get_db)
):
    """
    è·å–å¯ç”¨äºåŒ¹é…çš„ç®€å†åˆ—è¡¨ï¼ˆåŒ…å«å®Œæ•´ç»“æ„åŒ–æ•°æ®ç”¨äºé¢„è§ˆï¼‰
    """
    result = await db.execute(
        select(ResumeV2).order_by(ResumeV2.created_at.desc())
    )
    resumes = result.scalars().all()
    
    items = []
    for r in resumes:
        data = r.structured_data or {}
        
        # æå–æŠ€èƒ½åˆ—è¡¨
        skills = []
        skills_data = data.get('skills', {})
        if isinstance(skills_data, dict):
            for category in ['programming_languages', 'frameworks', 'tools', 'databases', 'other']:
                skills.extend(skills_data.get(category, []))
        elif isinstance(skills_data, list):
            skills = skills_data
        
        items.append({
            "id": r.id,
            "name": data.get('name', 'æœªçŸ¥'),
            "email": data.get('email'),
            "phone": data.get('phone'),
            "current_position": data.get('current_position'),
            "years_experience": data.get('years_experience'),
            "education": data.get('education'),
            "location": data.get('location'),
            "file_name": r.file_name,
            "extraction_method": r.extraction_method,
            "created_at": r.created_at.isoformat() if r.created_at else None,
            # å®Œæ•´ç»“æ„åŒ–æ•°æ®ç”¨äºé¢„è§ˆ
            "structured_data": data,
            # æ‰å¹³åŒ–çš„æŠ€èƒ½åˆ—è¡¨æ–¹ä¾¿å±•ç¤º
            "skills": skills[:15]
        })
    
    return {
        "total": len(resumes),
        "items": items
    }


@router.get("/keywords/{resume_id}")
async def get_resume_keywords(
    resume_id: int,
    db: AsyncSession = Depends(get_db)
):
    """
    è·å–ç®€å†çš„æ¨èæœç´¢å…³é”®è¯
    """
    result = await db.execute(select(ResumeV2).where(ResumeV2.id == resume_id))
    resume = result.scalar_one_or_none()
    if not resume:
        raise HTTPException(status_code=404, detail="ç®€å†ä¸å­˜åœ¨")
    
    keywords = extract_search_keywords(resume.structured_data or {})
    
    # è·å–ç®€å†ä¸­çš„åŸå¸‚
    resume_data = resume.structured_data or {}
    city = resume_data.get('location')
    if not city:
        job_intention = resume_data.get('job_intention', {})
        cities = job_intention.get('cities', [])
        city = cities[0] if cities else None
    
    return {
        "resume_id": resume_id,
        "name": resume_data.get('name', 'æœªçŸ¥'),
        "keywords": keywords,
        "suggested_city": city,
        "available_cities": list(BossWebCrawlerPlaywright.CITY_CODES.keys())
    }
