"""
çˆ¬è™«API - èŒä½æŠ“å–å’Œå…¥åº“ï¼ˆPlaywrightç‰ˆï¼Œæ”¯æŒCookieï¼‰
"""
from fastapi import APIRouter, HTTPException, Depends
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from typing import Optional
import asyncio
from pydantic import BaseModel

from backend.database.connection import get_db
from backend.crawlers.boss_web_crawler_playwright import BossWebCrawlerPlaywright
from backend.models.job_v2 import JobV2
from backend.services.extractor_service import ExtractorService
from backend.utils.local_embedding import LocalEmbeddingService
from backend.utils.logger import logger
from backend.config import settings

router = APIRouter(prefix="/api/crawler", tags=["çˆ¬è™«"])

extractor = ExtractorService()
embedding_service = LocalEmbeddingService()


class CrawlRequest(BaseModel):
    """çˆ¬è™«è¯·æ±‚"""
    keyword: str  # æœç´¢å…³é”®è¯
    city: str = "æ·±åœ³"  # åŸå¸‚
    max_results: int = 30  # æœ€å¤§ç»“æœæ•°ï¼ˆå¢åŠ é»˜è®¤å€¼ï¼‰
    fetch_detail: bool = True  # æ˜¯å¦è·å–è¯¦æƒ…é¡µ
    save_to_db: bool = True  # æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“
    max_concurrent: int = 5  # æœ€å¤§å¹¶å‘æ•°
    cookie_string: Optional[str] = None  # è‡ªå®šä¹‰Cookie
    auto_scroll: bool = True  # æ˜¯å¦æ»šåŠ¨åŠ è½½æ›´å¤š
    max_scroll: int = 5  # æœ€å¤§æ»šåŠ¨æ¬¡æ•°


class CrawlResponse(BaseModel):
    """çˆ¬è™«å“åº”"""
    total_found: int  # æ‰¾åˆ°çš„èŒä½æ•°
    saved_count: int  # ä¿å­˜çš„èŒä½æ•°
    skipped_count: int  # è·³è¿‡çš„èŒä½æ•°ï¼ˆå·²å­˜åœ¨ï¼‰
    failed_count: int  # å¤±è´¥çš„èŒä½æ•°
    jobs: list  # èŒä½åˆ—è¡¨


@router.post("/boss/search", response_model=CrawlResponse)
async def crawl_boss_jobs(
    request: CrawlRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    çˆ¬å–BOSSç›´è˜èŒä½ï¼ˆPlaywrightç‰ˆï¼Œæ”¯æŒCookieï¼‰
    
    Args:
        request: çˆ¬è™«è¯·æ±‚å‚æ•°
    
    Returns:
        çˆ¬å–ç»“æœç»Ÿè®¡å’ŒèŒä½åˆ—è¡¨
    """
    logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–BOSSç›´è˜: keyword={request.keyword}, city={request.city}")
    
    cookie = request.cookie_string or settings.BOSS_COOKIE
    
    if not cookie:
        raise HTTPException(
            status_code=400, 
            detail="æœªé…ç½®Cookieï¼Œè¯·é€šè¿‡ç¯å¢ƒå˜é‡BOSS_COOKIEè®¾ç½®æˆ–åœ¨è¯·æ±‚ä¸­ä¼ å…¥cookie_string"
        )
    
    async with BossWebCrawlerPlaywright(
        min_delay=1.5,
        max_delay=3.0,
        headless=True,
        cookie_string=cookie,
        target_city=request.city
    ) as crawler:
        try:
            # 1. æœç´¢èŒä½åˆ—è¡¨ï¼ˆæ ¹æ®å‚æ•°å†³å®šæ˜¯å¦æ»šåŠ¨ï¼‰
            all_jobs = await crawler.search_jobs(
                keyword=request.keyword,
                city=request.city,
                page=1,
                auto_scroll=request.auto_scroll,
                max_scroll=request.max_scroll
            )
            
            if not all_jobs:
                logger.warning("æœªçˆ¬å–åˆ°ä»»ä½•èŒä½")
                return CrawlResponse(
                    total_found=0,
                    saved_count=0,
                    skipped_count=0,
                    failed_count=0,
                    jobs=[]
                )
            
            # é™åˆ¶æ•°é‡
            jobs_to_fetch = all_jobs[:request.max_results]
            
            # 2. è·å–è¯¦æƒ…ï¼ˆå¯é€‰ï¼‰
            if request.fetch_detail:
                logger.info(f"ğŸ“¥ å¼€å§‹è·å– {len(jobs_to_fetch)} ä¸ªèŒä½çš„è¯¦æƒ…ï¼ˆå¹¶å‘æ•°: {request.max_concurrent}ï¼‰...")
                
                semaphore = asyncio.Semaphore(request.max_concurrent)
                
                async def fetch_detail(job, index):
                    async with semaphore:
                        job_url = job.get("job_url")
                        if not job_url:
                            return job
                        
                        detail = await crawler.get_job_detail(job_url, use_random_ua=True)
                        if detail:
                            job.update(detail)
                        
                        return job
                
                jobs = await asyncio.gather(
                    *[fetch_detail(job, idx) for idx, job in enumerate(jobs_to_fetch, 1)],
                    return_exceptions=True
                )
                
                # è¿‡æ»¤å¼‚å¸¸
                jobs = [j for j in jobs if not isinstance(j, Exception)]
            else:
                jobs = jobs_to_fetch
            
            logger.info(f"âœ… çˆ¬å–æˆåŠŸ: å…± {len(jobs)} ä¸ªèŒä½")
            logger.info(f"ğŸ“Š çˆ¬è™«ç»Ÿè®¡: {crawler.get_stats()}")
            
            # 3. ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¯é€‰ï¼‰
            saved_count = 0
            skipped_count = 0
            failed_count = 0
            
            if request.save_to_db:
                for job in jobs:
                    try:
                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                        job_id = job.get('job_id', '')
                        if job_id:
                            result = await db.execute(
                                select(JobV2).where(JobV2.external_id == job_id)
                            )
                            existing = result.scalar_one_or_none()
                            if existing:
                                logger.debug(f"èŒä½å·²å­˜åœ¨ï¼Œè·³è¿‡: {job.get('title')}")
                                skipped_count += 1
                                job['saved'] = False
                                job['reason'] = 'å·²å­˜åœ¨'
                                continue
                        
                        # æ„å»ºå®Œæ•´æè¿°
                        full_desc = job.get('job_description', '')
                        if not full_desc:
                            full_desc = f"{job.get('title', '')}\nå…¬å¸ï¼š{job.get('company', '')}\nè–ªèµ„ï¼š{job.get('salary', '')}"
                        
                        # æå–ç»“æ„åŒ–æ•°æ®
                        structured_data, confidence, method = extractor.extract_job(
                            text=full_desc,
                            use_llm=False,
                            force_llm=False
                        )
                        
                        # è¡¥å……åŸºæœ¬ä¿¡æ¯
                        structured_data['title'] = job.get('title', '')
                        structured_data['company'] = job.get('company', '')
                        structured_data['company_name'] = job.get('company_name', '')
                        structured_data['salary_range'] = job.get('salary_detail', job.get('salary', ''))
                        structured_data['job_keywords'] = job.get('job_keywords', [])
                        
                        # ç”Ÿæˆå‘é‡
                        try:
                            embedding = embedding_service.create_embedding(full_desc[:1000])
                        except:
                            embedding = None
                        
                        # ä¿å­˜
                        job_record = JobV2(
                            external_id=job_id,
                            platform="boss",
                            job_url=job.get('job_url', ''),
                            title=job.get('title', ''),
                            company_name=job.get('company_name', job.get('company', '')),
                            city=job.get('work_city', request.city),
                            salary_text=job.get('salary_detail', job.get('salary', '')),
                            experience_required=job.get('experience_requirement', job.get('experience', '')),
                            education_required=job.get('education_requirement', job.get('education', '')),
                            full_description=full_desc,
                            structured_data=structured_data,
                            extraction_method=method,
                            extraction_confidence=confidence,
                            description_embedding=embedding,
                            is_active=True
                        )
                        
                        db.add(job_record)
                        await db.commit()
                        await db.refresh(job_record)
                        
                        saved_count += 1
                        job['saved'] = True
                        job['db_id'] = job_record.id
                        logger.info(f"âœ… èŒä½ä¿å­˜æˆåŠŸ: {job.get('title')} (ID={job_record.id})")
                    
                    except Exception as e:
                        logger.error(f"âŒ ä¿å­˜èŒä½å¤±è´¥: {e}")
                        failed_count += 1
                        job['saved'] = False
                        job['reason'] = str(e)
            
            return CrawlResponse(
                total_found=len(all_jobs),
                saved_count=saved_count,
                skipped_count=skipped_count,
                failed_count=failed_count,
                jobs=jobs
            )
        
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–å¤±è´¥: {e}")
            raise HTTPException(status_code=500, detail=str(e))


@router.get("/boss/test")
async def test_boss_crawler():
    """
    æµ‹è¯•BOSSç›´è˜çˆ¬è™«ï¼ˆä¸ä¿å­˜æ•°æ®åº“ï¼‰
    
    Returns:
        æµ‹è¯•ç»“æœ
    """
    logger.info("ğŸ§ª æµ‹è¯•BOSSç›´è˜Playwrightçˆ¬è™«...")
    
    if not settings.BOSS_COOKIE:
        return {
            "success": False,
            "message": "æœªé…ç½®Cookieï¼Œè¯·é€šè¿‡ç¯å¢ƒå˜é‡BOSS_COOKIEè®¾ç½®",
            "jobs": [],
            "stats": {}
        }
    
    async with BossWebCrawlerPlaywright(
        min_delay=1.5,
        max_delay=3.0,
        headless=True,
        cookie_string=settings.BOSS_COOKIE,
        target_city="æ·±åœ³"
    ) as crawler:
        try:
            # æµ‹è¯•æœç´¢ï¼ˆåªçˆ¬3ä¸ªèŒä½ï¼‰
            jobs = await crawler.search_jobs(
                keyword="Python",
                city="æ·±åœ³",
                page=1,
                auto_scroll=False
            )
            
            if not jobs:
                return {
                    "success": False,
                    "message": "æœªæ‰¾åˆ°ä»»ä½•èŒä½",
                    "jobs": [],
                    "stats": crawler.get_stats()
                }
            
            # è·å–å‰3ä¸ªçš„è¯¦æƒ…
            jobs = jobs[:3]
            for job in jobs:
                job_url = job.get('job_url')
                if job_url:
                    detail = await crawler.get_job_detail(job_url, use_random_ua=True)
                    if detail:
                        job.update(detail)
            
            return {
                "success": True,
                "message": f"âœ… æµ‹è¯•æˆåŠŸï¼Œæ‰¾åˆ° {len(jobs)} ä¸ªèŒä½",
                "jobs": jobs,
                "stats": crawler.get_stats()
            }
        
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"æµ‹è¯•å¤±è´¥: {str(e)}",
                "jobs": [],
                "stats": crawler.get_stats()
            }


@router.get("/cities")
async def get_supported_cities():
    """
    è·å–æ”¯æŒçš„åŸå¸‚åˆ—è¡¨
    
    Returns:
        æ”¯æŒçš„åŸå¸‚åŠå…¶ä»£ç 
    """
    return {
        "cities": BossWebCrawlerPlaywright.CITY_CODES
    }
